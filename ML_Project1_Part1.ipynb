{"cells":[{"cell_type":"markdown","metadata":{"id":"R_IT7MbsYtJC"},"source":["# Introduction to Machine Learning\n","## Project Phase 1 - Spring Semester 1402-03\n","### Department of Electrical Engineering, Sharif University of Technology\n","### Instructor: Dr. R.Amiri\n","### Due Date: Tir 10, 1403 at 23:59\n","\n","## Machine Unlearning\n","\n","### Learning Phase\n","In this section, we will implement the SISA Algorithm and test its performance.\n","\n","1. **Sharding**: The training data is divided into S smaller subsets, or shards.\n","2. **Isolation**: Each shard is used to train a separate model or a component of a model independently.\n","3. **Slicing**: Each shard is further divided into slices, and training occurs in a sequential manner, slice by slice, within each shard.\n","4. **Aggregation**: The final model is constructed by aggregating the outputs from the independently trained shards."]},{"cell_type":"markdown","source":["**Part 1.1:**\n","\n","For learning phase:"],"metadata":{"id":"lBIx_9yMxcgp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OB9FZ1nzYtJE"},"outputs":[],"source":["# Import necessary libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader, random_split, ConcatDataset, Subset\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n","import numpy as np\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18098,"status":"ok","timestamp":1719958986370,"user":{"displayName":"Parsa Yousefpoor","userId":"15934681489068137527"},"user_tz":-210},"id":"KL86MeJhYtJF","outputId":"1c327c67-6569-4de7-c99b-95982fb10cec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:12<00:00, 13277446.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# Define the dataset and transformations\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","])\n","dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"]},{"cell_type":"markdown","source":["در اینجا داده‌ها را طبق الگوریتم سیسا به تعدادی قسمت و لایه تبدیل می‌کنیم."],"metadata":{"id":"F076Aii11TmT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fI1_aOkeYtJF"},"outputs":[],"source":["# Function to create shards and slices\n","def create_shards_and_slices(dataset, S, R):\n","    shards = random_split(dataset, [len(dataset) // S] * S)\n","    slices = [random_split(shard, [len(shard) // R] * R) for shard in shards]\n","    return slices"]},{"cell_type":"markdown","source":["در زیر کد برای این است که بر روی هر شارد و هر لایه‌ آن به ترتیب عمل عمل ترین را انجام دهیم.\n","\n","داده‌ها را 32تا 32تا لود کرده و الگوریتم ترین را انجام می‌دهیم.\n","\n","سپس بر طبق معیار گرادیان که آن را برابر صفر قرار می‌دهیم، به مدلمان یاد می‌دهیم."],"metadata":{"id":"czdsIrsS1moy"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eP09N_nFYtJF"},"outputs":[],"source":["# Function to train model on slices\n","def train_on_slices(S, R, model, slices, epochs):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    model.to(device)\n","    for i, slice_set in enumerate(slices):\n","        slice_dataset = ConcatDataset(slice_set)\n","        dataloader = DataLoader(slice_dataset, batch_size=32, shuffle=True)\n","        for epoch in range(epochs[i]):\n","            model.train()\n","            for images, labels in dataloader:\n","                images, labels = images.to(device), labels.to(device)\n","                optimizer.zero_grad()\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","        torch.save(model.state_dict(), f'model_slice_{S}_{R}_{i}.pth')\n","    return model"]},{"cell_type":"markdown","source":["در اینجا هم کدی برای گرفتن خروجی‌های مد نظر زده‌ایم.\n","\n","بر روی تمامی داده‌ها فرق بین خروجی مدل ما و لیبل وافعی را ثبت کرده و با توجه به آنها خروجی‌ها را به دست می‌آوریم.(با کمک توابع پیش فرض)"],"metadata":{"id":"5W03_Lc1EtyS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ts5MqxcOYtJF"},"outputs":[],"source":["# Define evaluation metrics\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    y_true, y_pred = [], []\n","    y_score = []\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted.cpu().numpy())\n","            y_score.extend(outputs.cpu().numpy())\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    y_true_one_hot = torch.nn.functional.one_hot(torch.tensor(y_true), num_classes=10).numpy()\n","    auroc = roc_auc_score(y_true_one_hot, y_score, multi_class='ovr')\n","    return f1, accuracy, precision, recall, auroc"]},{"cell_type":"markdown","source":["از اینجا به بعد برای هر کدام از حالات\n","\n","S\n","و\n","R\n","\n","برنامه را ران می‌کنیم."],"metadata":{"id":"TsDa1jQj0Q_r"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1504244,"status":"ok","timestamp":1719960521155,"user":{"displayName":"Parsa Yousefpoor","userId":"15934681489068137527"},"user_tz":-210},"id":"qImgWe0sYtJG","outputId":"cf2c5897-ef32-4460-f820-58e19c41ffdd"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 205MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1 Score: 0.8569020180441432, Accuracy: 0.8566, Precision: 0.8612518720489163, Recall: 0.8566, AUROC: 0.9839586666666668\n"]}],"source":["# Load dataset and create DataLoader\n","# 5 5\n","S = 5\n","R = 5\n","slices = create_shards_and_slices(dataset, S=5, R=5)\n","model1 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","model1.fc = nn.Linear(model1.fc.in_features, 10)  # Adjusting for CIFAR-10\n","model1 = train_on_slices(S, R, model1, slices, epochs=[10, 10, 10, 10, 10])\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model1, test_loader)\n","print(f'F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1530745,"status":"ok","timestamp":1719673641186,"user":{"displayName":"Parsa Yousefpoor","userId":"15934681489068137527"},"user_tz":-210},"id":"gamNEbeQKeBT","outputId":"8135d96f-8bac-46d0-97e6-a576c2b82e29"},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 Score: 0.8662552022576003, Accuracy: 0.8666, Precision: 0.8698110303826299, Recall: 0.8666, AUROC: 0.9862109222222222\n"]}],"source":["# 5 10\n","S,R = 5,10\n","slices = create_shards_and_slices(dataset, S=5, R=10)\n","model2 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","model2.fc = nn.Linear(model2.fc.in_features, 10)  # Adjusting for CIFAR-10\n","model2 = train_on_slices(S, R, model2, slices, epochs=[10, 10, 10, 10, 10])\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model2, test_loader)\n","print(f'F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uPECL9cgKnim","outputId":"91c1cf14-7cdb-4e10-d4f7-d416d4fd3ad9","executionInfo":{"status":"ok","timestamp":1719727507218,"user_tz":-210,"elapsed":1558315,"user":{"displayName":"amirebrahim yusefpour","userId":"07597639241589250640"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["F1 Score: 0.8557526062540055, Accuracy: 0.8554, Precision: 0.8591821335757857, Recall: 0.8554, AUROC: 0.9833812222222222\n"]}],"source":["# 5 20\n","S,R = 5,20\n","slices = create_shards_and_slices(dataset, S=5, R=20)\n","model3 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","model3.fc = nn.Linear(model3.fc.in_features, 10)  # Adjusting for CIFAR-10\n","model3 = train_on_slices(S, R, model3, slices, epochs=[10, 10, 10, 10, 10])\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model3, test_loader)\n","print(f'F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5X0hgjp8KyWB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719725878702,"user_tz":-210,"elapsed":1567364,"user":{"displayName":"amirebrahim yusefpour","userId":"07597639241589250640"}},"outputId":"3d5847b9-34ff-4ae9-9cc0-175d72c4d2ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","F1 Score: 0.8480197688269827, Accuracy: 0.848, Precision: 0.849958646261961, Recall: 0.848, AUROC: 0.980687811111111\n"]}],"source":["# 10 5\n","S,R = 10,5\n","slices = create_shards_and_slices(dataset, S=10, R=5)\n","model4 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","model4.fc = nn.Linear(model4.fc.in_features, 10)  # Adjusting for CIFAR-10\n","model4 = train_on_slices(S, R, model4, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model4, test_loader)\n","print(f'F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vyjsHUDfKyEv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719930802294,"user_tz":-210,"elapsed":1579849,"user":{"displayName":"amirebrahim yusefpour","userId":"07597639241589250640"}},"outputId":"6658c011-11be-48c7-a0d1-36d752343b18"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 72.5MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1 Score: 0.8353651532954307, Accuracy: 0.8361, Precision: 0.8373842597494878, Recall: 0.8361, AUROC: 0.9803153388888889\n"]}],"source":["# 10 10\n","S,R = 10,10\n","slices = create_shards_and_slices(dataset, S=10, R=10)\n","model5 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","model5.fc = nn.Linear(model5.fc.in_features, 10)  # Adjusting for CIFAR-10\n","model5 = train_on_slices(S, R, model5, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model5, test_loader)\n","print(f'F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"idACH6E1Kx9B"},"outputs":[],"source":["# 10 20\n","S,R = 10,20\n","slices = create_shards_and_slices(dataset, S=10, R=20)\n","model6 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","model6.fc = nn.Linear(model6.fc.in_features, 10)  # Adjusting for CIFAR-10\n","model6 = train_on_slices(S, R, model6, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model6, test_loader)\n","print(f'F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mateU5TpK_lR"},"outputs":[],"source":["# 20 5\n","S,R = 20,5\n","slices = create_shards_and_slices(dataset, S=20, R=5)\n","model7 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","model7.fc = nn.Linear(model7.fc.in_features, 10)  # Adjusting for CIFAR-10\n","model7 = train_on_slices(S, R, model7, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model7, test_loader)\n","print(f'F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeVQXEIbK_hx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c5987b3-9205-4f22-b536-dfd77737e9bd"},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 139MB/s]\n"]}],"source":["# 20 10\n","S,R = 20,10\n","slices = create_shards_and_slices(dataset, S=20, R=10)\n","model8 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","model8.fc = nn.Linear(model8.fc.in_features, 10)  # Adjusting for CIFAR-10\n","model8 = train_on_slices(S, R, model8, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model8, test_loader)\n","print(f'F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WGWG5XCGK_dK"},"outputs":[],"source":["# 20 20\n","S,R = 20,20\n","slices = create_shards_and_slices(dataset, S=20, R=20)\n","model9 = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","model9.fc = nn.Linear(model9.fc.in_features, 10)  # Adjusting for CIFAR-10\n","model9 = train_on_slices(S, R, model9, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model9, test_loader)\n","print(f'F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"markdown","source":["**Part1.2:**\n","\n","For unlearning phase."],"metadata":{"id":"fauK1JGb9WBO"}},{"cell_type":"markdown","source":["کدی که زدیم، تا حدی شبیه کد برای یادگیری اولیه است.\n","\n","با این تفاوت که اگر فرقی در داده‌های آن اسلایس نبود، مدل ما ری‌اکشنی نشان نداده و ادامه می‌دهد."],"metadata":{"id":"gTadez2oGaOU"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sP9cMnJRYtJG"},"outputs":[],"source":["# Unlearning Phase\n","# In this phase, we will implement the unlearning algorithm on all trained models and remove 500 randomly chosen data points.\n","\n","# Function to unlearn data\n","def unlearn_data(S, R, model, data_to_forget, slices, epochs):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    model.to(device)\n","    for i, slice_set in enumerate(slices):\n","        updated_slice = ConcatDataset([data for data in slice_set if data not in data_to_forget])\n","        dataloader = DataLoader(updated_slice, batch_size=32, shuffle=True)\n","        for epoch in range(epochs[i]):\n","            model.train()\n","            for images, labels in dataloader:\n","                images, labels = images.to(device), labels.to(device)\n","                optimizer.zero_grad()\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","        torch.save(model.state_dict(), f'unlearned_model_slice_{S}_{R}_{i}.pth')\n","    return model\n","# Select 500 data points to forget\n","data_to_forget, other_dataset = random_split(dataset, [500, len(dataset) - 500])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNKp2039YtJG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719962874797,"user_tz":-210,"elapsed":1521396,"user":{"displayName":"Parsa Yousefpoor","userId":"15934681489068137527"}},"outputId":"a8d65b51-f852-4cda-b046-4a59e16f1859"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unlearned Model - F1 Score: 0.8666689852835648, Accuracy: 0.8647, Precision: 0.8753282073280295, Recall: 0.8647, AUROC: 0.9871240666666667\n"]}],"source":["# 5 5\n","S,R = 5,5\n","model11 = unlearn_data(S, R, model1, data_to_forget, slices, epochs=[10, 10, 10, 10, 10])\n","f1, accuracy, precision, recall, auroc = evaluate_model(model1, test_loader)\n","print(f'Unlearned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1553300,"status":"ok","timestamp":1719675578204,"user":{"displayName":"Parsa Yousefpoor","userId":"15934681489068137527"},"user_tz":-210},"id":"xlS-NHIULnHS","outputId":"7a45f08a-2c22-45c5-f7a2-6db58de7f89a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unlearned Model - F1 Score: 0.8717827619842392, Accuracy: 0.87, Precision: 0.883633839706258, Recall: 0.87, AUROC: 0.987616\n"]}],"source":["# 5 10\n","S,R = 5,10\n","model21 = unlearn_data(S, R, model2, data_to_forget, slices, epochs=[10, 10, 10, 10, 10])\n","f1, accuracy, precision, recall, auroc = evaluate_model(model2, test_loader)\n","print(f'Unlearned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcyKKPeGLm9U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719729127771,"user_tz":-210,"elapsed":1616394,"user":{"displayName":"amirebrahim yusefpour","userId":"07597639241589250640"}},"outputId":"80550dd3-7c56-4a41-bbe5-16bc782fcb64"},"outputs":[{"output_type":"stream","name":"stdout","text":["Unlearned Model - F1 Score: 0.8797749882376206, Accuracy: 0.8806, Precision: 0.8802754874917887, Recall: 0.8806, AUROC: 0.9879218444444444\n"]}],"source":["# 5 20\n","S,R = 5,20\n","model31 = unlearn_data(S, R, model3, data_to_forget, slices, epochs=[10, 10, 10, 10, 10])\n","f1, accuracy, precision, recall, auroc = evaluate_model(model3, test_loader)\n","print(f'Unlearned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1556287,"status":"ok","timestamp":1719679854404,"user":{"displayName":"Parsa Yousefpoor","userId":"15934681489068137527"},"user_tz":-210},"id":"l53T_7I2Lm0A","outputId":"5ece7c6e-0d09-4bde-e7a0-1dec55304516"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unlearned Model - F1 Score: 0.8585644084380872, Accuracy: 0.8577, Precision: 0.8682500900764355, Recall: 0.8577, AUROC: 0.9853614111111112\n"]}],"source":["# 10 5\n","S,R = 10,5\n","model41 = unlearn_data(S, R, model4, data_to_forget, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","f1, accuracy, precision, recall, auroc = evaluate_model(model4, test_loader)\n","print(f'Unlearned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9XVtNzQLmqF"},"outputs":[],"source":["# 10 10\n","S,R = 10,10\n","model51 = unlearn_data(S, R, model5, data_to_forget, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","f1, accuracy, precision, recall, auroc = evaluate_model(model5, test_loader)\n","print(f'Unlearned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZM5Fnqk0Lmgk"},"outputs":[],"source":["# 10 20\n","S,R = 10,20\n","model61 = unlearn_data(S, R, model6, data_to_forget, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","f1, accuracy, precision, recall, auroc = evaluate_model(mode6, test_loader)\n","print(f'Unlearned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxUw-wt3LmWl"},"outputs":[],"source":["# 20 5\n","S,R = 20,5\n","model71 = unlearn_data(S, R, model7, data_to_forget, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","f1, accuracy, precision, recall, auroc = evaluate_model(model7, test_loader)\n","print(f'Unlearned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJcJprudLmMg"},"outputs":[],"source":["# 20 10\n","S,R = 20,10\n","model81 = unlearn_data(S, R, model8, data_to_forget, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","f1, accuracy, precision, recall, auroc = evaluate_model(model8, test_loader)\n","print(f'Unlearned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0JmSXBRLmCR"},"outputs":[],"source":["# 20 20\n","S,R = 20,20\n","model91 = unlearn_data(S, R, model9, data_to_forget, slices, epochs=[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n","f1, accuracy, precision, recall, auroc = evaluate_model(mode9, test_loader)\n","print(f'Unlearned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')"]},{"cell_type":"markdown","source":["**Part 1.3:**\n","\n","For example we use train model and untrain model of S=5 & R=5."],"metadata":{"id":"ckRdzuzTCRt3"}},{"cell_type":"markdown","source":["در اینجا توابعی که نیاز بوده را آورده‌ایم.\n","\n","مانند\n","\n","1. compute loss\n","2. train attacker model\n","3. ...\n","\n","که همه این توابع در بخش بعد هم توضیح داده شده‌اند."],"metadata":{"id":"DaLaEpW8HUrM"}},{"cell_type":"code","source":["# Membership Inference Attack\n","def membership_inference_attack(losses_train, losses_test):\n","    from sklearn.linear_model import LogisticRegression\n","    from sklearn.model_selection import cross_val_score\n","    X = losses_train + losses_test\n","    y = [1] * len(losses_train) + [0] * len(losses_test)\n","    clf = LogisticRegression(random_state=0).fit(X, y)\n","    return clf\n","\n","def compute_losses(model, dataset):\n","    model.eval()\n","    losses = []\n","    criterion = nn.CrossEntropyLoss()\n","    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            losses.append(loss.item())\n","    return losses\n","\n","def train_attacker_model(losses_train, losses_test):\n","    from sklearn.linear_model import LogisticRegression\n","    X = losses_train + losses_test\n","    y = [1] * len(losses_train) + [0] * len(losses_test)\n","    clf = LogisticRegression(random_state=0).fit([[x] for x in X], y)\n","    return clf\n","\n","def cross_val_score(clf, X, y, cv=5):\n","    from sklearn.model_selection import StratifiedKFold\n","    skf = StratifiedKFold(n_splits=cv)\n","    scores = []\n","    for train_index, test_index in skf.split(X, y):\n","        X_train, X_test = [X[i] for i in train_index], [X[i] for i in test_index]\n","        y_train, y_test = [y[i] for i in train_index], [y[i] for i in test_index]\n","        clf.fit([[x] for x in X_train], y_train)\n","        score = clf.score([[x] for x in X_test], y_test)\n","        scores.append(score)\n","    return sum(scores) / len(scores)"],"metadata":{"id":"UwCugdIIRnik"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["دیتاست 1: بخشی از دیتاست ترین با وجود داده‌های فراموشی. کل داده‌ها را در نظر گرفتیم.\n","\n","دیتاست 2: یک بخشی از دیتاست تست. test_subset1\n","\n","دیتاست 3: بخشی از دیتاست ترین بدون وجود داده‌های فراموشی. کل داده‌ها به جز داده‌های فراموشی.\n","\n","دیتاست 4: بخش دیگری از دیتاست تست. test_subset2"],"metadata":{"id":"UUjqkBfjShhE"}},{"cell_type":"code","source":["split_ratio = 0.5  # 50% split\n","split_size = int(len(test_dataset) * split_ratio)\n","remaining_size = len(test_dataset) - split_size\n","\n","# Split the test_dataset into two subsets\n","test_subset1, test_subset2 = random_split(test_dataset, [split_size, remaining_size])"],"metadata":{"id":"XNivPaLvRpCT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1)\tلاس‌های دیتاست 1 و 2 در مدل ترین شده و قبل از unlearn.\n","\n","2)\tلاس‌های دیتاست 3 و 4 در مدل ترین شده و قبل از unlearn."],"metadata":{"id":"mJFDjuL3SeaY"}},{"cell_type":"code","source":["baseline_losses_train_1 = compute_losses(model1, dataset)\n","baseline_losses_test_1 = compute_losses(model1, test_subset1)\n","baseline_losses_train_2 = compute_losses(model1, other_dataset)\n","baseline_losses_test_2 = compute_losses(model1, test_subset2)\n","\n","# Combine and cross-validate for baseline model\n","clf_baseline_1 = train_attacker_model(baseline_losses_train_1, baseline_losses_test_1)\n","clf_baseline_2 = train_attacker_model(baseline_losses_train_2, baseline_losses_test_2)\n","\n","X_baseline_1 = baseline_losses_train_1 + baseline_losses_test_1\n","y_baseline_1 = [1] * len(baseline_losses_train_1) + [0] * len(baseline_losses_test_1)\n","cv_score_baseline_1 = cross_val_score(clf_baseline_1, X_baseline_1, y_baseline_1)\n","\n","X_baseline_2 = baseline_losses_train_2 + baseline_losses_test_2\n","y_baseline_2 = [1] * len(baseline_losses_train_2) + [0] * len(baseline_losses_test_2)\n","cv_score_baseline_2 = cross_val_score(clf_baseline_2, X_baseline_2, y_baseline_2)\n","\n","# Print the cross-validation scores\n","print(\"Cross-validation score for baseline model (subset 1):\", cv_score_baseline_1)\n","print(\"Cross-validation score for baseline model (subset 2):\", cv_score_baseline_2)"],"metadata":{"id":"-ScinzyMRw3W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719960685266,"user_tz":-210,"elapsed":143179,"user":{"displayName":"Parsa Yousefpoor","userId":"15934681489068137527"}},"outputId":"da49942f-daad-43fa-b538-0e12b839d410"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-validation score for baseline model (subset 1): 0.9143229256511989\n","Cross-validation score for baseline model (subset 2): 0.907563025210084\n"]}]},{"cell_type":"markdown","source":["3)\tلاس‌های دیتاست 1 و 2 در مدل بعد از unlearn.\n","\n","4)\tلاس‌های دیتاست 3 و 4 در مدل بعد از unlearn."],"metadata":{"id":"GPRxlnY9SbUz"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":144287,"status":"ok","timestamp":1719963025942,"user":{"displayName":"Parsa Yousefpoor","userId":"15934681489068137527"},"user_tz":-210},"id":"OCtW6qTzYtJH","outputId":"c31e1e3f-9413-4d4f-b906-b4083ce7b30c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-validation score for privacy model (subset 1): 0.9336915646023805\n","Cross-validation score for privacy model (subset 2): 0.907563025210084\n"]}],"source":["privacy_losses_train_1 = compute_losses(model11, dataset)\n","privacy_losses_test_1 = compute_losses(model11, test_subset1)\n","privacy_losses_train_2 = compute_losses(model11, other_dataset)\n","privacy_losses_test_2 = compute_losses(model11, test_subset2)\n","\n","# Combine and cross-validate for privacy model\n","clf_privacy_1 = train_attacker_model(privacy_losses_train_1, privacy_losses_test_1)\n","clf_privacy_2 = train_attacker_model(privacy_losses_train_2, privacy_losses_test_2)\n","\n","X_privacy_1 = privacy_losses_train_1 + privacy_losses_test_1\n","y_privacy_1 = [1] * len(privacy_losses_train_1) + [0] * len(privacy_losses_test_1)\n","cv_score_privacy_1 = cross_val_score(clf_privacy_1, X_privacy_1, y_privacy_1)\n","\n","X_privacy_2 = privacy_losses_train_2 + privacy_losses_test_2\n","y_privacy_2 = [1] * len(privacy_losses_train_2) + [0] * len(privacy_losses_test_2)\n","cv_score_privacy_2 = cross_val_score(clf_privacy_2, X_privacy_2, y_privacy_2)\n","\n","# Print the cross-validation scores\n","print(\"Cross-validation score for privacy model (subset 1):\", cv_score_privacy_1)\n","print(\"Cross-validation score for privacy model (subset 2):\", cv_score_privacy_2)"]},{"cell_type":"markdown","source":["Part 1.4"],"metadata":{"id":"2TgbbtBZ9Gfw"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms, models\n","from torch.utils.data import DataLoader, random_split, Subset\n","from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, roc_auc_score\n","import numpy as np\n","import random\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define the dataset and transformations\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","])\n","dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","# Function to create shards and slices\n","def create_shards_and_slices(dataset, S, R):\n","    shards = random_split(dataset, [len(dataset) // S] * S)\n","    slices = [random_split(shard, [len(shard) // R] * R) for shard in shards]\n","    return slices\n","\n","# Function to train model on slices\n","def train_on_slices(model, slices, epochs):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    model.to(device)\n","    for i, slice_set in enumerate(slices):\n","        slice_dataset = ConcatDataset(slice_set)\n","        dataloader = DataLoader(slice_dataset, batch_size=32, shuffle=True)\n","        for epoch in range(epochs[i]):\n","            model.train()\n","            for images, labels in dataloader:\n","                images, labels = images.to(device), labels.to(device)\n","                optimizer.zero_grad()\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","        torch.save(model.state_dict(), f'model_slice_{i}.pth')\n","    return model\n","\n","# Define evaluation metrics\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    y_true, y_pred = [], []\n","    y_score = []\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted.cpu().numpy())\n","            y_score.extend(outputs.cpu().numpy())\n","    f1 = f1_score(y_true, y_pred, average='weighted')\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred, average='weighted')\n","    recall = recall_score(y_true, y_pred, average='weighted')\n","    y_true_one_hot = torch.nn.functional.one_hot(torch.tensor(y_true), num_classes=10).numpy()\n","    auroc = roc_auc_score(y_true_one_hot, y_score, multi_class='ovr')\n","    return f1, accuracy, precision, recall, auroc\n","\n","# Function to poison the dataset\n","def poison_data(dataset, class_to_poison, num_samples, block_size=(3, 3)):\n","    indices = [i for i, (_, label) in enumerate(dataset) if label == class_to_poison]\n","    selected_indices = random.sample(indices, num_samples)\n","    poisoned_data = []\n","    for idx in range(len(dataset)):\n","        image, label = dataset[idx]\n","        if idx in selected_indices:\n","            image = image.numpy()\n","            c, h, w = image.shape\n","            x = random.randint(0, w - block_size[1])\n","            y = random.randint(0, h - block_size[0])\n","            image[:, y:y + block_size[0], x:x + block_size[1]] = 0\n","            image = torch.tensor(image)\n","        poisoned_data.append((image, label))\n","    return poisoned_data, selected_indices\n","\n","# Function to unlearn data\n","def unlearn_data(model, data_to_forget, epochs):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","    model.to(device)\n","    dataloader = DataLoader(data_to_forget, batch_size=32, shuffle=True)\n","    for epoch in range(epochs):\n","        model.train()\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","    return model\n","\n","# Load dataset and create DataLoader\n","class_to_poison = 3  # Example class to poison\n","poisoned_data, poisoned_indices = poison_data(dataset, class_to_poison, num_samples=500)\n","poisoned_dataset = Subset(poisoned_data, range(len(poisoned_data)))\n","\n","slices = create_shards_and_slices(poisoned_dataset, S=5, R=5)\n","model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","model.fc = nn.Linear(model.fc.in_features, 10)  # Adjusting for CIFAR-10\n","model = train_on_slices(model, slices, epochs=[10, 10, 10, 10, 10])\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model, test_loader)\n","print(f'Poisoned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')\n","\n","# Calculate ASR\n","def calculate_asr(model, test_loader, target_class):\n","    model.eval()\n","    total_samples = 0\n","    misclassified_as_target = 0\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            total_samples += labels.size(0)\n","            misclassified_as_target += (predicted == target_class).sum().item()\n","    asr = misclassified_as_target / total_samples * 100\n","    return asr\n","\n","asr = calculate_asr(model, test_loader, target_class=class_to_poison)\n","print(f'Attack Success Rate (ASR): {asr}%')\n","\n","# Unlearn the poisoned data and evaluate again\n","data_to_forget = Subset(poisoned_dataset, poisoned_indices)\n","model = unlearn_data(model, data_to_forget, epochs=10)\n","f1, accuracy, precision, recall, auroc = evaluate_model(model, test_loader)\n","print(f'Unlearned Model - F1 Score: {f1}, Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, AUROC: {auroc}')\n","\n","# Calculate ASR after unlearning\n","asr = calculate_asr(model, test_loader, target_class=class_to_poison)\n","print(f'Attack Success Rate (ASR) after unlearning: {asr}%')"],"metadata":{"id":"anHCthb_9G-7","outputId":"c79b59a6-3ee7-485c-fd0e-7bccfa3fb3de","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"error","timestamp":1719994883885,"user_tz":-210,"elapsed":52610,"user":{"displayName":"Parsa Yousefpoor","userId":"04031230405880116583"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:01<00:00, 99923154.77it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 95.7MB/s]\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'ConcatDataset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b12033008642>\u001b[0m in \u001b[0;36m<cell line: 109>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet18_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMAGENET1K_V1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adjusting for CIFAR-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_on_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-b12033008642>\u001b[0m in \u001b[0;36mtrain_on_slices\u001b[0;34m(model, slices, epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_set\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mslice_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ConcatDataset' is not defined"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}